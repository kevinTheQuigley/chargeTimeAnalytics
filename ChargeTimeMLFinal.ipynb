{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "625f4b16",
   "metadata": {},
   "source": [
    "# Charge Time Machine Learning Algorithm Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d853284",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "* [1.Reading Data](#read)\n",
    "  * [1.1 Train-Test Split](#split)\n",
    "  * [1.2 Data Normalization](#normal)\n",
    "  * [1.3 Dataframe Variations](#dfs)\n",
    "\n",
    "\n",
    "* [2.Data Analysis](#split)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efbcb81",
   "metadata": {},
   "source": [
    "## Reading Data <a name=\"read\"></a>\n",
    "\n",
    "### Splitting the data <a name=\"split\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8def5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as td\n",
    "import  numpy as np\n",
    "import datetime \n",
    "import datetime as dt\n",
    "import math as ma\n",
    "df = pd.read_csv('data/chargeML.csv')\n",
    "df=df._convert(numeric=True)\n",
    "df=df.dropna()\n",
    "#df['date2'] =  pd.to_datetime(df['date2'])\n",
    "#df.index.to_pydatetime() \n",
    "#df=df.date2.to_pydatetime() \n",
    "df['date2']=pd.to_datetime(df.date2)\n",
    "#df['date2'] =  pd.to_datetime(df['date2'])\n",
    "df[\"dayInMonth\"] =df['date2'].dt.day\n",
    "inData=dataSet=df\n",
    "## Need to add a cos function for day of week\n",
    "\n",
    "df[\"cos_week\"] = np.cos(df[\"dayOfWeek\"]/7*2*ma.pi)\n",
    "df[\"sin_hour\"]=np.sin(df[\"dayOfWeek\"]/7*2*ma.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80decb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "splitOption = 1 # split per day of the month\n",
    "\n",
    "#cutOffTestDate=2022.01.01\n",
    "cutOffTestDate = datetime.datetime(2022, 3, 1)\n",
    "testSet = dataSet.loc[(inData.date2 > cutOffTestDate), :]\n",
    "mainSet = dataSet.loc[(inData.date2 <= cutOffTestDate), :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8066827",
   "metadata": {},
   "source": [
    "### Normalising the data  <a name=\"normal\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4043a9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52086/1205477737.py:23: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  trainSet=trainSet.drop('date2',1)\n",
      "/tmp/ipykernel_52086/1205477737.py:24: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  validSet=validSet.drop('date2',1)\n",
      "/tmp/ipykernel_52086/1205477737.py:25: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  testSet=testSet.drop('date2',1)\n",
      "/tmp/ipykernel_52086/1205477737.py:26: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  trainSet=trainSet.drop('dayInMonth',1)\n",
      "/tmp/ipykernel_52086/1205477737.py:27: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  validSet=validSet.drop('dayInMonth',1)\n",
      "/tmp/ipykernel_52086/1205477737.py:28: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  testSet=testSet.drop('dayInMonth',1)\n",
      "/tmp/ipykernel_52086/1205477737.py:29: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  trainSet=trainSet.drop('dayOfWeek',1)\n",
      "/tmp/ipykernel_52086/1205477737.py:30: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  validSet=validSet.drop('dayOfWeek',1)\n",
      "/tmp/ipykernel_52086/1205477737.py:31: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  testSet=testSet.drop('dayOfWeek',1)\n",
      "/tmp/ipykernel_52086/1205477737.py:32: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  trainSet=trainSet.drop('dayOfYear',1)\n",
      "/tmp/ipykernel_52086/1205477737.py:33: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  validSet=validSet.drop('dayOfYear',1)\n",
      "/tmp/ipykernel_52086/1205477737.py:34: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  testSet=testSet.drop('dayOfYear',1)\n",
      "/tmp/ipykernel_52086/1205477737.py:35: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  trainSet=trainSet.drop('hour',1)\n",
      "/tmp/ipykernel_52086/1205477737.py:36: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  validSet=validSet.drop('hour',1)\n",
      "/tmp/ipykernel_52086/1205477737.py:37: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  testSet=testSet.drop('hour',1)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data by the 23rd of each month, to allow for subsequent hours to be used for predictions.\n",
    "#Data can also be split by a single cut off date too\n",
    "\n",
    "# Trainset is the untrasformed data\n",
    "trainSet = dataSet.loc[(dataSet.dayInMonth > 0) & (dataSet.dayInMonth < 23), :]\n",
    "validSet = dataSet.loc[(dataSet.dayInMonth >= 23) & (inData.date2 <= cutOffTestDate), :]\n",
    "\n",
    "# Choosing a predcited column\n",
    "predCol =['ActualWind']\n",
    "\n",
    "\n",
    "# Saving these columns in case \n",
    "ogWindTrain = trainSet['ActualWind']\n",
    "ogWindValid = validSet['ActualWind']\n",
    "\n",
    "\n",
    "#Creating the trainDate column, so it can be re-added later\n",
    "trainDate=trainSet.date2\n",
    "validDate=validSet.date2\n",
    "testDate=testSet.date2\n",
    "\n",
    "# Removing the date Column\n",
    "trainSet=trainSet.drop('date2',1)\n",
    "validSet=validSet.drop('date2',1)\n",
    "testSet=testSet.drop('date2',1)\n",
    "trainSet=trainSet.drop('dayInMonth',1)\n",
    "validSet=validSet.drop('dayInMonth',1)\n",
    "testSet=testSet.drop('dayInMonth',1)\n",
    "trainSet=trainSet.drop('dayOfWeek',1)\n",
    "validSet=validSet.drop('dayOfWeek',1)\n",
    "testSet=testSet.drop('dayOfWeek',1)\n",
    "trainSet=trainSet.drop('dayOfYear',1)\n",
    "validSet=validSet.drop('dayOfYear',1)\n",
    "testSet=testSet.drop('dayOfYear',1)\n",
    "trainSet=trainSet.drop('hour',1)\n",
    "validSet=validSet.drop('hour',1)\n",
    "testSet=testSet.drop('hour',1)\n",
    "\n",
    "\n",
    "\n",
    "#When transforming the data in the future, we need to be able to transform the complete forecast,\n",
    "# for all four stations at once\n",
    "usedColsForecast=['dub_wddir','dub_wdsp','dub_temp','dub_msl',\n",
    "    'don_wddir','don_wdsp','don_temp','don_msl',\n",
    "    'cla_wddir','cla_wdsp','cla_temp','cla_msl',\n",
    "    'cor_wddir','cor_wdsp','cor_temp','cor_msl']\n",
    "\n",
    "# Two versions are saved, to allow for the transformed wind-vector columns to be used\n",
    "usedColsForecastVec=['dub_wddir','dub_wdsp','dub_temp','dub_msl',\n",
    "    'don_wddir','don_wdsp','don_temp','don_msl',\n",
    "    'cla_wddir','cla_wdsp','cla_temp','cla_msl',\n",
    "    'cor_wddir','cor_wdsp','cor_temp','cor_msl',\n",
    "    'cos_dub_vec', 'sin_dub_vec',\n",
    "    'cos_cor_vec', 'sin_cor_vec', 'cos_cla_vec', \n",
    "    'sin_cla_vec','cos_don_vec', 'sin_don_vec']\n",
    "\n",
    "# Same as usecColsForecast, but includes time variables\n",
    "allCols=['sin_day','sin_hour','cos_day','cos_hour',\n",
    "    'dub_wddir','dub_wdsp','dub_temp','dub_msl',\n",
    "    'don_wddir','don_wdsp','don_temp','don_msl',\n",
    "    'cla_wddir','cla_wdsp','cla_temp','cla_msl',\n",
    "    'cor_wddir','cor_wdsp','cor_temp','cor_msl']\n",
    "\n",
    "# Same as usecColsForecast, but includes time variables\n",
    "allCols_Vec=['dub_wddir','dub_wdsp','dub_temp','dub_msl',\n",
    "    'don_wddir','don_wdsp','don_temp','don_msl',\n",
    "    'cla_wddir','cla_wdsp','cla_temp','cla_msl',\n",
    "    'cor_wddir','cor_wdsp','cor_temp','cor_msl',\n",
    "    'cos_dub_vec', 'sin_dub_vec',\n",
    "    'cos_cor_vec', 'sin_cor_vec', 'cos_cla_vec', \n",
    "    'sin_cla_vec','cos_don_vec', 'sin_don_vec']\n",
    "\n",
    "usedColsSelect=['sin_day','sin_hour','cos_day','cos_hour',\n",
    "                'dub_wddir','dub_wdsp','don_wdsp','cor_wdsp',\n",
    "                'cla_wdsp','dub_msl','don_wdsp','cos_cor_wddir',\n",
    "                'sin_cor_wddir','dub_temp','cos_dub_wddir','sin_dub_wddir']\n",
    "\n",
    "usedColsBasic=['sin_day','sin_hour','cos_day','cos_hour',\n",
    "               'dub_wddir','dub_wdsp','dub_msl']\n",
    "\n",
    "\n",
    "\n",
    "train_mean = trainSet.mean()\n",
    "train_std = trainSet.std()\n",
    "\n",
    "\n",
    "train_meanSave=train_mean[usedColsForecast]\n",
    "train_stdSave =train_std[usedColsForecast]\n",
    "\n",
    "# Saving columns which are used to transform data used in future predictive modelling\n",
    "np.savetxt(\"./models/train_meanSave.csv\", train_meanSave, delimiter=\",\")\n",
    "np.savetxt(\"./models/train_stdSave.csv\", train_meanSave, delimiter=\",\")\n",
    "\n",
    "train_transformed = (trainSet - train_mean) / train_std\n",
    "valid_transformed = (validSet - train_mean) / train_std\n",
    "test_transformed = (testSet - train_mean) / train_std\n",
    "\n",
    "\n",
    "train_meanSaveVec=train_mean[usedColsForecastVec]\n",
    "train_stdSaveVec =train_std[usedColsForecastVec]\n",
    "\n",
    "# Saving columns which are used to transform data used in future predictive modelling\n",
    "np.savetxt(\"./models/train_meanSaveVec.csv\", train_meanSave, delimiter=\",\")\n",
    "np.savetxt(\"./models/train_stdSaveVec.csv\", train_meanSave, delimiter=\",\")\n",
    "\n",
    "#re-creating date-column\n",
    "train_transformed['date2']=trainDate\n",
    "valid_transformed['date2']=validDate\n",
    "test_transformed['date2']=testDate\n",
    "\n",
    "# Re-adding removed columns\n",
    "train_transformed['hour']=train_transformed['date2'].dt.hour\n",
    "train_transformed['dayOfWeek']=train_transformed['date2'].dt.day_of_week\n",
    "train_transformed['dayOfYear']=train_transformed['date2'].dt.day_of_year\n",
    "train_transformed[\"dayInMonth\"] =train_transformed['date2'].dt.day\n",
    "# Re-adding removed columns\n",
    "valid_transformed['hour']=valid_transformed['date2'].dt.hour\n",
    "valid_transformed['dayOfWeek']=valid_transformed['date2'].dt.day_of_week\n",
    "valid_transformed['dayOfYear']=valid_transformed['date2'].dt.day_of_year\n",
    "valid_transformed[\"dayInMonth\"] =valid_transformed['date2'].dt.day\n",
    "# Re-adding removed columns\n",
    "test_transformed['hour']=test_transformed['date2'].dt.hour\n",
    "test_transformed['dayOfWeek']=test_transformed['date2'].dt.day_of_week\n",
    "test_transformed['dayOfYear']=test_transformed['date2'].dt.day_of_year\n",
    "test_transformed[\"dayInMonth\"] =test_transformed['date2'].dt.day\n",
    "\n",
    "#Resetting the predicted  column to not be a transformed variable\n",
    "train_transformed[predCol]=train_transformed[predCol]*train_std[predCol]+train_mean[predCol]\n",
    "valid_transformed[predCol]=valid_transformed[predCol]*train_std[predCol]+train_mean[predCol]\n",
    "test_transformed[predCol]=test_transformed[predCol]*train_std[predCol]+train_mean[predCol]\n",
    "\n",
    "# Re-setting the names of the columns\n",
    "train_transformed\n",
    "valid_transformed\n",
    "testSet=test_transformed\n",
    "\n",
    "\n",
    "y_trainWind = trainSet['ActualWind']\n",
    "y_validWind = validSet['ActualWind']\n",
    "\n",
    "\n",
    "y_testWind = testSet[predCol]\n",
    "X_train = trainSet\n",
    "X_valid = validSet \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da84b32",
   "metadata": {},
   "source": [
    "## Dataframe Generation  <a name=\"dfs\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5dd724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating dataframes\n",
    "#Version 1:\n",
    "X_trainAll = train_transformed[allCols]\n",
    "X_validAll = valid_transformed[allCols]\n",
    "\n",
    "X_trainAllVec = train_transformed[allCols_Vec]\n",
    "X_validAllVec = test_transformed[allCols_Vec]\n",
    "\n",
    "\n",
    "#Simplified Version:\n",
    "X_trainBasic = train_transformed[usedColsBasic]\n",
    "X_validBasic = test_transformed[usedColsBasic]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff054ba",
   "metadata": {},
   "source": [
    "## Random forest generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b5cd6",
   "metadata": {},
   "source": [
    "### Random Forest Reggresion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8849ed3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[1;32m      3\u001b[0m rrfAll\u001b[38;5;241m=\u001b[39mRandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, max_leaf_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mrrfAll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trainAll\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_trainWind\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:476\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    465\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    468\u001b[0m ]\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    187\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 189\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py:1342\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \n\u001b[1;32m   1316\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1342\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    449\u001b[0m         splitter,\n\u001b[1;32m    450\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    456\u001b[0m     )\n\u001b[0;32m--> 458\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rrfAll=RandomForestRegressor(n_estimators=500,random_state = 42, max_depth=400, max_leaf_nodes=400)\n",
    "\n",
    "rrfAll.fit(X_trainAll,y_trainWind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predRRFAll=rrfAll.predict(X_validAll)\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "plt.plot(X_valid['date2'][-6*24:],y_predRRFAll[-6*24:],color = 'blue',label = 'RFF Predicted Wind Geneartion')\n",
    "plt.plot(X_valid['date2'][-6*24:],y_valid[-6*24:],color = 'red',label = 'Actual Wind Generation')\n",
    "plt\n",
    "fig = plt.gcf()\n",
    "plt.legend()\n",
    "#plt.size(7,7)\n",
    "fig.set_size_inches(11, 7)\n",
    "\n",
    "fig.savefig('MLImages/rForestForecast2.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ef186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_valid, y_predRRFAll))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_valid, y_predRRFAll))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_valid, y_predRRFAll)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded3073",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(X_validAll.columns, rrfAll.feature_importances_)\n",
    "plt.title(\"Random Forest Example Feature Importantences\")\n",
    "plt.savefig(\"MLImages/RforestImportances1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "rrfBasic=RandomForestRegressor(n_estimators=200,random_state = 42)\n",
    "rrfBasic.fit(X_trainBasic,y_trainWind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predRffBasic=rrfBasic.predict(X_validBasic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "plt.plot(X_valid['date2'][-6*24:],y_predRffBasic[-6*24:],color = 'blue')\n",
    "plt.plot(X_valid['date2'][-6*24:],y_valid[-6*24:],color = 'red')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "fig.savefig('randomForestVec.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20edf149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_valid,y_predRffBasic))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_valid, y_predRffBasic))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_valid, y_predRffBasic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b92912",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(X_validBasic.columns, rrf2.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b54fc76",
   "metadata": {},
   "source": [
    "This:-\n",
    "rrf=RandomForestRegressor(n_estimators=100,random_state = 42, max_depth=100, max_leaf_nodes=100)\n",
    "\n",
    "Yields this:-\n",
    "Mean Absolute Error: 465.79800324008335\n",
    "Mean Squared Error: 332345.30211887945\n",
    "Root Mean Squared Error: 576.4939740525302\n",
    "\n",
    "This:-\n",
    "rrf=RandomForestRegressor(n_estimators=100,random_state = 42, max_depth=100, max_leaf_nodes=100)\n",
    "\n",
    "Yields this:-\n",
    "Mean Absolute Error: 479.4909188704617\n",
    "Mean Squared Error: 362932.07708576124\n",
    "Root Mean Squared Error: 602.438442569663"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbaa718",
   "metadata": {},
   "source": [
    "## Creating a Baseline Model Using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_decision_forests as tfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSetTF=trainSet\n",
    "validSetTF=validSet\n",
    "testSetTF=testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running into difficulty converting these columns, so I reset the value of X_train to include ActualGenerationMW\n",
    "label = predCol[0]\n",
    "'''\n",
    "y_trainTF = tfdf.keras.pd_dataframe_to_tf_dataset(y_trainTF, label=label, task=tfdf.keras.Task.REGRESSION)\n",
    "y_validTF = tfdf.keras.pd_dataframe_to_tf_dataset(y_validTF, label=label, task=tfdf.keras.Task.REGRESSION)\n",
    "y_testTF = tfdf.keras.pd_dataframe_to_tf_dataset(y_testTF, label=label, task=tfdf.keras.Task.REGRESSION)\n",
    "'''\n",
    "\n",
    "X_trainAllTF = tfdf.keras.pd_dataframe_to_tf_dataset(trainSet[allCols+predCol], label=label, task=tfdf.keras.Task.REGRESSION)\n",
    "\n",
    "X_validAllTF = tfdf.keras.pd_dataframe_to_tf_dataset(validSetTF[allCols+predCol], label=label, task=tfdf.keras.Task.REGRESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using keras\n",
    "model_7 = tfdf.keras.RandomForestModel(task = tfdf.keras.Task.REGRESSION)\n",
    "\n",
    "# Train the model.\n",
    "model_7.fit(x=X_trainAllTF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Another TF model\n",
    "\n",
    "using_col_list = ['ForecastWind','sin_day','sin_hour','cos_day','cos_hour','dub_wddir','dub_wdsp','dub_msl','ActualGenerationMW']\n",
    "using_col_list.remove('ActualGenerationMW')\n",
    "feature_list = []\n",
    "for col in using_col_list:\n",
    "    feature_list.append(tfdf.keras.FeatureUsage(name=col,semantic=tfdf.keras.FeatureSemantic.NUMERICAL))\n",
    "\n",
    "\n",
    "model8 = tfdf.keras.RandomForestModel(features = feature_list, task = tfdf.keras.Task.REGRESSION,\n",
    "    exclude_non_specified_features=True,\n",
    "    num_trees=1000, max_depth=16,\n",
    "    split_axis=\"SPARSE_OBLIQUE\",categorical_algorithm=\"RANDOM\",\n",
    "    missing_value_policy='RANDOM_LOCAL_IMPUTATION',\n",
    "    sparse_oblique_normalization='STANDARD_DEVIATION',\n",
    "    compute_oob_variable_importances=True,\n",
    "    winner_take_all=False)\n",
    "                                                \n",
    "\n",
    "#model8.compile(metrics=[\"Accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a32c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wurlitzer import sys_pipes\n",
    "with sys_pipes():\n",
    "    model8.fit(X_trainTF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c795833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure what test_ds should be\n",
    "'''\n",
    "# Evaluate the model on the test dataset.\n",
    "model8.compile(metrics=[\"mse\"])\n",
    "evaluation = model8.evaluate(test_ds, return_dict=True)\n",
    "\n",
    "print(evaluation)\n",
    "print()\n",
    "print(f\"MSE: {evaluation['mse']}\")\n",
    "print(f\"RMSE: {math.sqrt(evaluation['mse'])}\")\n",
    "#print(f\"MAE: {evaluation['mae']}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4274bf",
   "metadata": {},
   "source": [
    "Not sure if I'll get much better then above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dcc2a2",
   "metadata": {},
   "source": [
    "## Tenserflow Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be52e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_with_predCol = train_transformed[allCols+predCol]\n",
    "val_df_with_predCol = valid_transformed[allCols+predCol]\n",
    "test_df_with_predCol= test_transformed[allCols+predCol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05af7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df_with_predCol = train_df_with_predCol\n",
    "    self.val_df_with_predCol = val_df_with_predCol\n",
    "    self.test_df_with_predCol= test_df3\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df_with_predCol.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df_with_predCol)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.val_df_with_predCol)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_dataset(self.test_df3)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.train))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "  if self.label_columns is not None:\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d35ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(self, model=None, plot_col='ActualGenerationMW', max_subplots=3):\n",
    "  inputs, labels = self.example\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plot_col_index = self.column_indices[plot_col]\n",
    "  max_n = min(max_subplots, len(inputs))\n",
    "  for n in range(max_n):\n",
    "    plt.subplot(max_n, 1, n+1)\n",
    "    plt.ylabel(f'{plot_col} [normed]')\n",
    "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "    if self.label_columns:\n",
    "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "    else:\n",
    "      label_col_index = plot_col_index\n",
    "\n",
    "    if label_col_index is None:\n",
    "      continue\n",
    "\n",
    "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "    if model is not None:\n",
    "      predictions = model(inputs)\n",
    "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "\n",
    "    if n == 0:\n",
    "      plt.legend()\n",
    "\n",
    "  plt.xlabel('Time [h]')\n",
    "\n",
    "WindowGenerator.plot = plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517a1a17",
   "metadata": {},
   "source": [
    "\n",
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f09a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "\n",
    "n = len(df)\n",
    "train_df = df[0:int(n*0.7)]\n",
    "val_df = df[int(n*0.7):int(n*0.9)]\n",
    "test_df = df[int(n*0.9):]\n",
    "\n",
    "num_features = df.shape[1]\n",
    "single_step_window = WindowGenerator(\n",
    "    input_width=1, label_width=1, shift=1,\n",
    "    label_columns=['ActualGenerationMW'])\n",
    "single_step_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78460d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(self, data):\n",
    "  data = np.array(data, dtype=np.float32)\n",
    "  ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "      data=data,\n",
    "      targets=None,\n",
    "      sequence_length=self.total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=True,\n",
    "      batch_size=32,)\n",
    "\n",
    "  ds = ds.map(self.split_window)\n",
    "\n",
    "  return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ba1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(tf.keras.Model):\n",
    "  def __init__(self, label_index=None):\n",
    "    super().__init__()\n",
    "    self.label_index = label_index\n",
    "\n",
    "  def call(self, inputs):\n",
    "    if self.label_index is None:\n",
    "      return inputs\n",
    "    result = inputs[:, :, self.label_index]\n",
    "    return result[:, :, tf.newaxis]\n",
    "\n",
    "baseline = Baseline(label_index=column_indices['ActualGenerationMW'])\n",
    "\n",
    "baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                 metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "val_performance = {}\n",
    "performance = {}\n",
    "val_performance['Baseline'] = baseline.evaluate(single_step_window.val)\n",
    "performance['Baseline'] = baseline.evaluate(single_step_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window = WindowGenerator(\n",
    "    input_width=24, label_width=24, shift=1,\n",
    "    label_columns=['ActualGenerationMW'])\n",
    "\n",
    "wide_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input shape:', wide_window.example[0].shape)\n",
    "print('Output shape:', baseline(wide_window.example[0]).shape)\n",
    "wide_window.plot(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce9ccad",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# I think this could be important,\n",
    "# If convulutional width is set to 3, then given 3 hours of inputs it predicts 1 hour into the future\n",
    "# We want for \n",
    "CONV_WIDTH = 3\n",
    "\n",
    "conv_window = WindowGenerator(\n",
    "    input_width=CONV_WIDTH,\n",
    "    label_width=1,\n",
    "    shift=1,\n",
    "    label_columns=['ActualGenerationMW'])\n",
    "\n",
    "conv_window'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6534cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''conv_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32,\n",
    "                           kernel_size=(CONV_WIDTH,),\n",
    "                           activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518629d",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca25cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "ann = tf.keras.models.Sequential([            \n",
    "          tf.keras.layers.Dense(units=111, activation='relu', name=\"Layer_1\"),\n",
    "          tf.keras.layers.Dropout(0.1), #drop-out layer to avoid overfit\n",
    "          tf.keras.layers.Dense(units=111, activation='relu', name=\"Layer_2\"),\n",
    "          tf.keras.layers.Dropout(0.1),\n",
    "          tf.keras.layers.Dense(units=111, activation='relu', name=\"Layer_3\"),\n",
    "          tf.keras.layers.Dropout(0.1),\n",
    "          tf.keras.layers.Dense(units=10, activation='relu', name=\"Layer_4\"),\n",
    "          tf.keras.layers.Dense(units=1, name=\"output_layer\")\n",
    "          ])\n",
    "\n",
    "ann.compile(loss=tf.keras.losses.mae, # mae is short for mean absolute error\n",
    "            optimizer=tf.keras.optimizers.Adam(lr=0.00002),\n",
    "            metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying stuff from document...\n",
    "epochs=60\n",
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.8\n",
    "\n",
    "#sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "ann.compile(loss=tf.keras.losses.mae, # mae is short for mean absolute error\n",
    "            optimizer=tf.keras.optimizers.Adam(lr=0.00002),\n",
    "            metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2c446",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainWind= trainSet.ActualWind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2282213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ann.fit(X_trainAll,y_trainWind batch_size=20, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predANN=ann.predict(X_validAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_valid, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_valid, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_valid, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99240c2e",
   "metadata": {},
   "source": [
    "For lr=0.002 and  batch_size=200, epochs=100)\n",
    "\n",
    "Mean Absolute Error: 470.42779251935326\n",
    "Mean Squared Error: 353878.8971401352\n",
    "Root Mean Squared Error: 594.877211817813\n",
    "\n",
    "For lr=0.0002 and  batch_size=200, epochs=100)\n",
    "Mean Absolute Error: 478.03476671201486\n",
    "Mean Squared Error: 364369.589799041\n",
    "Root Mean Squared Error: 603.6303420132565\n",
    "\n",
    "For lr=0.0002 and  batch_size=200, epochs=150)\n",
    "Mean Absolute Error: 470.0360041216103\n",
    "Mean Squared Error: 354797.59158343874\n",
    "Root Mean Squared Error: 595.648882802141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "plt.plot(X_valid['date2'][-7*24:],y_pred[-7*24:])\n",
    "plt.plot(X_valid['date2'][-7*24:],y_valid[-7*24:])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "fig.savefig('test2png.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.barh(X_validAll.columns, ann.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42907a15",
   "metadata": {},
   "source": [
    "### Single hour ANN Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714589a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainSingle.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann1 = tf.keras.models.Sequential([            \n",
    "        tf.keras.layers.Dense(units=120, activation='relu', name=\"Layer_1\"),\n",
    "        tf.keras.layers.Dropout(0.1), #drop-out layer to avoid overfit\n",
    "        tf.keras.layers.Dense(units=120, activation='relu', name=\"Layer_2\"),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(units=120, activation='relu', name=\"Layer_3\"),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(units=10, activation='relu', name=\"Layer_4\"),\n",
    "        tf.keras.layers.Dense(units=1, name=\"output_layer\")\n",
    "        ])\n",
    "\n",
    "ann1.compile(loss=tf.keras.losses.mae, # mae is short for mean absolute error\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=0.002),\n",
    "          metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "\n",
    "sinTrain1=train_df[allCols]\n",
    "y_trainSingle=train_df[predCol]\n",
    "\n",
    "sinValid1=val_df[allCols]\n",
    "y_validSingle=val_df[predCol]\n",
    "\n",
    "history = ann1.fit(sinTrain1, y_trainSingle, epochs = 27, validation_data=(sinValid1,y_validSingle), verbose=0)\n",
    "# history = ann.fit(X_train_normal, y_trainWind epochs = 20, validation_data=(X_valid_normal,y_valid), callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1,len(loss)+1)\n",
    "\n",
    "plt.plot(epochs,loss,'bo',label='Training loss')\n",
    "plt.plot(epochs,val_loss, label='Validation loss')\n",
    "plt.title('Training loss ')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (weatherMerged['date2'] > '2022-6-21') & (weatherMerged['date2'] <= '2022-6-28')\n",
    "print(weatherMerged['ActualWind'].loc[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dad4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_predS=ann1.predict(sinValid1)\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_validSingle, y_predS))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_validSingle, y_predS))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_validSingle, y_predS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f024e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann1.save('models/ann_modelS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241318bc",
   "metadata": {},
   "source": [
    "### Using a different set of columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892d0b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann1 = tf.keras.models.Sequential([            \n",
    "        tf.keras.layers.Dense(units=120, activation='relu', name=\"Layer_1\"),\n",
    "        tf.keras.layers.Dropout(0.1), #drop-out layer to avoid overfit\n",
    "        tf.keras.layers.Dense(units=120, activation='relu', name=\"Layer_2\"),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(units=120, activation='relu', name=\"Layer_3\"),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(units=10, activation='relu', name=\"Layer_4\"),\n",
    "        tf.keras.layers.Dense(units=1, name=\"output_layer\")\n",
    "        ])\n",
    "\n",
    "ann1.compile(loss=tf.keras.losses.mae, # mae is short for mean absolute error\n",
    "          optimizer=tf.keras.optimizers.Adam(lr=0.002),\n",
    "          metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_trainSingle=train_df[predCol]\n",
    "y_validSingle=val_df[predCol]\n",
    "\n",
    "sinTrain1=train_df[allColsForecastVec]\n",
    "sinValid1=val_df[allColsForecastVec]\n",
    "\n",
    "\n",
    "history = ann1.fit(sinTrain1, y_trainSingle, epochs = 30, validation_data=(sinValid1,y_validSingle), verbose=0)\n",
    "# history = ann.fit(X_train_normal, y_trainWind epochs = 20, validation_data=(X_valid_normal,y_valid), callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b95261",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1,len(loss)+1)\n",
    "\n",
    "plt.plot(epochs,loss,'bo',label='Training loss')\n",
    "plt.plot(epochs,val_loss, label='Validation loss')\n",
    "plt.title('Training loss ')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f202e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8bb3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b057a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.save('models/ann_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b461a12",
   "metadata": {},
   "source": [
    "## Muliple hour input Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd698af",
   "metadata": {},
   "source": [
    "Below from https://www.tensorflow.org/tutorials/structured_data/time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3abd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = WindowGenerator(input_width=6, label_width=1, shift=0,\n",
    "                     label_columns=['ActualGenerationMW'])\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c68c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Stack three slices, the length of the total window.\n",
    "example_window = tf.stack([np.array(train_df_with_predCol[:w2.total_window_size]),\n",
    "                           np.array(train_df_with_predCol[100:100+w2.total_window_size]),\n",
    "                           np.array(train_df_with_predCol[200:200+w2.total_window_size])])\n",
    "\n",
    "example_inputs, example_labels = w2.split_window(example_window)\n",
    "\n",
    "print('All shapes are: (batch, time, features)')\n",
    "print(f'Window shape: {example_window.shape}')\n",
    "print(f'Inputs shape: {example_inputs.shape}')\n",
    "print(f'Labels shape: {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54664ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_WIDTH = 3\n",
    "conv_window = WindowGenerator(\n",
    "    input_width=CONV_WIDTH,\n",
    "    label_width=1,\n",
    "    shift=0,\n",
    "    label_columns=['ActualGenerationMW'])\n",
    "\n",
    "conv_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93296280",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_window.plot()\n",
    "plt.title(\"Given 3 hours of inputs, predict 1 hour into the future.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fcbda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 20\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=patience,mode='min')\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "  return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce1451b",
   "metadata": {},
   "source": [
    "### Multi-step dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52572ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_dense = tf.keras.Sequential([\n",
    "    # Shape: (time, features) => (time*features)\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "    # Add back the time dimension.\n",
    "    # Shape: (outputs) => (1, outputs)\n",
    "    tf.keras.layers.Reshape([1, -1]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b66da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_and_fit(multi_step_dense, conv_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['Multi step dense'] = multi_step_dense.evaluate(conv_window.val)\n",
    "performance['Multi step dense'] = multi_step_dense.evaluate(conv_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5eff77",
   "metadata": {},
   "source": [
    "### conv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eba86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32,kernel_size=(CONV_WIDTH,),activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_and_fit(conv_model, conv_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['Conv'] = conv_model.evaluate(conv_window.val)\n",
    "performance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a479b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_window.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea66cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 20\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=patience,mode='min')\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "  print(window.train)\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7756abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_with_predCol = train_df[allCols+predCol]\n",
    "val_df_with_predCol = val_df[allCols+predCol]\n",
    "test_df_with_predCol= test_df[allCols+predCol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df_with_predCol, val_df=val_df_with_predCol, test_df=test_df3,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df_with_predCol = train_df_with_predCol\n",
    "    self.val_df_with_predCol = val_df_with_predCol\n",
    "    self.test_df_with_predCol= test_df3\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df_with_predCol.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "    print(\"self.input_indices are given by \",self.input_indices)\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "##----------------------->\n",
    "CONV_WIDTH = 24\n",
    "conv_window = WindowGenerator(\n",
    "    input_width=CONV_WIDTH,\n",
    "    label_width=0,\n",
    "    shift=0,\n",
    "    label_columns=['ActualGenerationMW'])\n",
    "\n",
    "conv_window\n",
    "\n",
    "##----------------------->\n",
    "\n",
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df_with_predCol)\n",
    "\n",
    "\n",
    "@property\n",
    "def trainX(self):\n",
    "  return self.make_dataset(self.train_df_with_predColX)\n",
    "\n",
    "@property\n",
    "def trainY(self):\n",
    "  return self.make_dataset(self.train_df_with_predColY)\n",
    "\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.val_df_with_predCol)\n",
    "\n",
    "@property\n",
    "def valX(self):\n",
    "  return self.make_dataset(self.val_df_with_predColX)\n",
    "\n",
    "@property\n",
    "def valY(self):\n",
    "  return self.make_dataset(self.val_df_with_predColY)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_dataset(self.test_df3)\n",
    "\n",
    "@property\n",
    "def testX(self):\n",
    "  return self.make_dataset(self.test_df3X)\n",
    "\n",
    "@property\n",
    "def testY(self):\n",
    "  return self.make_dataset(self.test_df3Y)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.train))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "\n",
    "WindowGenerator.val = val\n",
    "\n",
    "\n",
    "WindowGenerator.test = test\n",
    "\n",
    "\n",
    "\n",
    "WindowGenerator.example = example\n",
    "###================================>\n",
    "\n",
    "def split_window(self, features):\n",
    "    print(\"self.input_slice is \",self.input_slice)\n",
    "    print(\"self.labels_slice is \",self.labels_slice)\n",
    "    inputs = features[:, self.input_slice, :]\n",
    "    labels = features[:, self.labels_slice, :]\n",
    "    print(\"labels are given by\", labels)\n",
    "    print(\"inputs are given by\", inputs)\n",
    "\n",
    "    '''if self.label_columns is not None:\n",
    "        labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)'''\n",
    "\n",
    "    # Slicing doesn't preserve static shape information, so set the shapes\n",
    "    # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "    inputs.set_shape([None, self.input_width, None])\n",
    "    labels.set_shape([None, self.label_width, None])\n",
    "    print(\"labels are now given by\", labels)\n",
    "    print(\"inputs are now given by\", inputs)\n",
    "    return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window\n",
    "\n",
    "###================================>\n",
    "\n",
    "def make_dataset(self, data):\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    print(\"Data is  given by\",data.shape)\n",
    "    \n",
    "    inputData=data[:,:20]\n",
    "    targets=data[:,20:]\n",
    "    print(\"Input data is given by\", inputData.shape)\n",
    "    print(\"targets are given by\",targets.shape)\n",
    "\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        inputData,\n",
    "        targets,\n",
    "        sequence_length=self.total_window_size,\n",
    "        sequence_stride=1,\n",
    "        shuffle=False,\n",
    "        batch_size=32,)\n",
    "    print(\"ds is given by \",ds)\n",
    "    #ds = ds.map(self.split_window)\n",
    "\n",
    "    return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset\n",
    "\n",
    "###================================>\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=patience,mode='min')\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "  print(window.train)\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "  return history\n",
    "\n",
    "###================================>\n",
    "\n",
    "conv_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32,kernel_size=(CONV_WIDTH-1,),activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "])\n",
    "###================================>\n",
    "\n",
    "history = compile_and_fit(conv_model, conv_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['Conv'] = conv_model.evaluate(conv_window.val)\n",
    "performance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9910cf20",
   "metadata": {},
   "source": [
    "#### Generating basic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ff355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "y_predRFF=rrf.predict(X_validAll)\n",
    "\n",
    "y_valid =        val_df[predCol].iloc[2:]\n",
    "y_forecasted = val_df['ForecastWind'].iloc[2:]\n",
    "\n",
    "\n",
    "y_predAr=conv_model.predict(conv_window.val)\n",
    "y_pred=y_predAr[:,0]\n",
    "y_forecast = validSet.ForecastWind\n",
    "\n",
    "\n",
    "y_valid=val_df_with_predCol[predCol]\n",
    "#og here\n",
    "\n",
    "\n",
    "plt.plot(X_valid['date2'][-6*24:],y_pred[-6*24:],color = 'cyan',label = 'ANN Modeled Wind Generation')\n",
    "plt.plot(X_valid['date2'][-6*24:],y_valid[-6*24:],color = 'red',label = 'Actual Wind Generation')\n",
    "plt.plot(X_valid['date2'][-6*24:],y_forecast[-6*24:],color = 'blue', label = 'ESB Forecasted Wind Generation')\n",
    "plt.plot(X_valid['date2'][-6*24:],y_predRFF[-6*24:],color = 'purple',label = 'RFF Predicted Wind Geneartion')\n",
    "\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.legend(loc=1)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 7)\n",
    "\n",
    "fig.savefig('MLImages/ModelWeekDisplay.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923c8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "#og stuff\n",
    "y_valid =        validSet.ForecastWind.iloc[2:]\n",
    "y_forecasted = validSet.ActualWind.iloc[2:]\n",
    "\n",
    "MAEAN2=metrics.mean_absolute_error(y_valid, y_pred)\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_valid, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_valid, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_valid, y_pred)))\n",
    "\n",
    "\n",
    "y_valid2 = validSet['ActualWind']\n",
    "print('\\n')\n",
    "MAEFOR=metrics.mean_absolute_error(y_valid2, y_predRFF)\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_valid2, y_predRFF))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_valid2, y_predRFF))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_valid2, y_predRFF)))\n",
    "\n",
    "MAERFF=metrics.mean_absolute_error(y_valid, y_forecasted)\n",
    "print('\\n')\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_valid, y_forecasted))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_valid, y_forecasted))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_valid, y_forecasted)))\n",
    "\n",
    "MAEANN=metrics.mean_absolute_error(y_valid2, y_predANN)\n",
    "print('\\n')\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_valid2, y_predANN))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_valid2, y_predANN))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_valid2, y_predANN)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "width = 0.25\n",
    "\n",
    "\n",
    "plt.bar(x = 1, height = MAEAN2, color = 'b', width = width, edgecolor = 'black',label='MAE Multi hour')\n",
    "plt.bar(x = 2, height = MAEANN, color = 'r', width = width, edgecolor = 'black',label='MAE single hour ANN')\n",
    "plt.bar(x = 3, height = MAEFOR, color = 'purple', width = width, edgecolor = 'purple',label='MAE  Random Forest')\n",
    "\n",
    "plt.bar(x = 4, height = MAERFF, color = 'cyan', width = width, edgecolor = 'purple',label='MAE Forecast Wind')\n",
    "\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Mean Absolute Error\")\n",
    "plt.title(\"Accuracy of various ML Algorithms\")\n",
    "\n",
    "# plt.grid(linestyle='--')\n",
    "#plt.xticks(r + width/2,['2018','2019','2020','2021'])\n",
    "plt.legend(loc=[0,0])\n",
    "plt.savefig('MLImages/AccuracyScores.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4eb70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d5809",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.save('models/conv_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e95640",
   "metadata": {},
   "source": [
    "#### Can I get a Hell yeah\n",
    "_Helia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d2e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(self, model=None, plot_col='ActualGenerationMW', max_subplots=3):\n",
    "  inputs, labels = self.example\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plot_col_index = self.column_indices[plot_col]\n",
    "  max_n = min(max_subplots, len(inputs))\n",
    "  for n in range(max_n):\n",
    "    plt.subplot(max_n, 1, n+1)\n",
    "    plt.ylabel(f'{plot_col} [normed]')\n",
    "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "    if self.label_columns:\n",
    "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "    else:\n",
    "      label_col_index = plot_col_index\n",
    "\n",
    "    if label_col_index is None:\n",
    "      continue\n",
    "\n",
    "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "    if model is not None:\n",
    "      predictions = model(inputs)\n",
    "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "\n",
    "    if n == 0:\n",
    "      plt.legend()\n",
    "\n",
    "  plt.xlabel('Time [h]')\n",
    "\n",
    "WindowGenerator.plot = plot\n",
    "\n",
    "\n",
    "\n",
    "LABEL_WIDTH = 24\n",
    "INPUT_WIDTH = LABEL_WIDTH + (CONV_WIDTH - 1)\n",
    "wide_conv_window = WindowGenerator(\n",
    "    input_width=INPUT_WIDTH,\n",
    "    label_width=LABEL_WIDTH,\n",
    "    shift=1,\n",
    "    label_columns=['ActualGenerationMW'])\n",
    "\n",
    "wide_conv_window\n",
    "\n",
    "wide_conv_window.plot(conv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06897190",
   "metadata": {},
   "outputs": [],
   "source": [
    "adsf;ligb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42434075",
   "metadata": {},
   "outputs": [],
   "source": [
    "usedCols=['sin_day','sin_hour','cos_day','cos_hour','dub_wddir','dub_wdsp','dub_temp','dub_msl',\n",
    "    'don_wddir','don_wdsp','don_temp','don_msl',\n",
    "    'cla_wddir','cla_wdsp','cla_temp','cla_msl',\n",
    "    'cor_wddir','cor_wdsp','cor_temp','cor_msl','ActualWind']\n",
    "\n",
    "\n",
    "train_df_with_predCol = train_df[usedCols]\n",
    "val_df_with_predCol = val_df[usedCols]\n",
    "test_df_with_predCol= test_df[usedCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df_with_predCol, val_df=val_df_with_predCol, test_df=test_df3,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df_with_predCol = train_df_with_predCol\n",
    "    self.val_df_with_predCol = val_df_with_predCol\n",
    "    self.test_df_with_predCol= test_df3\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df_with_predCol.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "    print(\"self.input_indices are given by \",self.input_indices)\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "##----------------------->\n",
    "CONV_WIDTH = 3\n",
    "conv_window = WindowGenerator(\n",
    "    input_width=CONV_WIDTH,\n",
    "    label_width=0,\n",
    "    shift=0,\n",
    "    label_columns=['ActualGenerationMW'])\n",
    "\n",
    "conv_window\n",
    "\n",
    "##----------------------->\n",
    "\n",
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df_with_predCol)\n",
    "\n",
    "\n",
    "@property\n",
    "def trainX(self):\n",
    "  return self.make_dataset(self.train_df_with_predColX)\n",
    "\n",
    "@property\n",
    "def trainY(self):\n",
    "  return self.make_dataset(self.train_df_with_predColY)\n",
    "\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.val_df_with_predCol)\n",
    "\n",
    "@property\n",
    "def valX(self):\n",
    "  return self.make_dataset(self.val_df_with_predColX)\n",
    "\n",
    "@property\n",
    "def valY(self):\n",
    "  return self.make_dataset(self.val_df_with_predColY)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_dataset(self.test_df3)\n",
    "\n",
    "@property\n",
    "def testX(self):\n",
    "  return self.make_dataset(self.test_df3X)\n",
    "\n",
    "@property\n",
    "def testY(self):\n",
    "  return self.make_dataset(self.test_df3Y)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.train))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "\n",
    "WindowGenerator.val = val\n",
    "\n",
    "\n",
    "WindowGenerator.test = test\n",
    "\n",
    "\n",
    "\n",
    "WindowGenerator.example = example\n",
    "###================================>\n",
    "\n",
    "def split_window(self, features):\n",
    "    print(\"self.input_slice is \",self.input_slice)\n",
    "    print(\"self.labels_slice is \",self.labels_slice)\n",
    "    inputs = features[:, self.input_slice, :]\n",
    "    labels = features[:, self.labels_slice, :]\n",
    "    print(\"labels are given by\", labels)\n",
    "    print(\"inputs are given by\", inputs)\n",
    "\n",
    "    '''if self.label_columns is not None:\n",
    "        labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)'''\n",
    "\n",
    "    # Slicing doesn't preserve static shape information, so set the shapes\n",
    "    # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "    inputs.set_shape([None, self.input_width, None])\n",
    "    labels.set_shape([None, self.label_width, None])\n",
    "    print(\"labels are now given by\", labels)\n",
    "    print(\"inputs are now given by\", inputs)\n",
    "    return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window\n",
    "\n",
    "###================================>\n",
    "\n",
    "def make_dataset(self, data):\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    print(\"Data is  given by\",data.shape)\n",
    "    \n",
    "    inputData=data[:,:20]\n",
    "    targets=data[:,20:]\n",
    "    print(\"Input data is given by\", inputData.shape)\n",
    "    print(\"targets are given by\",targets.shape)\n",
    "\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        inputData,\n",
    "        targets,\n",
    "        sequence_length=self.total_window_size,\n",
    "        sequence_stride=1,\n",
    "        shuffle=False,\n",
    "        batch_size=32,)\n",
    "    print(\"ds is given by \",ds)\n",
    "    #ds = ds.map(self.split_window)\n",
    "\n",
    "    return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset\n",
    "\n",
    "###================================>\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=patience,mode='min')\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "  print(window.train)\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "  return history\n",
    "\n",
    "###================================>\n",
    "\n",
    "conv_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32,kernel_size=(CONV_WIDTH-1,),activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "])\n",
    "###================================>\n",
    "\n",
    "history = compile_and_fit(conv_model, conv_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['Conv'] = conv_model.evaluate(conv_window.val)\n",
    "performance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6963254",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_WIDTH = 24\n",
    "INPUT_WIDTH = LABEL_WIDTH + (CONV_WIDTH - 1)\n",
    "wide_conv_window = WindowGenerator(\n",
    "    input_width=INPUT_WIDTH,\n",
    "    label_width=LABEL_WIDTH,\n",
    "    shift=1,\n",
    "    label_columns=['ActualGenerationMW'])\n",
    "\n",
    "wide_conv_window\n",
    "\n",
    "wide_conv_window.plot(conv_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9affe71d",
   "metadata": {},
   "source": [
    "### Transforming new data for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b41a20",
   "metadata": {},
   "source": [
    "### Saving the model in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13104496",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.save('models/conv_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e26bf4",
   "metadata": {},
   "source": [
    "## Comparing baselines - Predicting wind power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30841dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f3219",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_with_predCol = train_df[allCols+predCol]\n",
    "val_df_with_predCol = val_df[allCols+predCol]\n",
    "test_df_with_predCol= test_df[allCols+predCol]\n",
    "\n",
    "#Resetting the Actual Generation to be non-continuous\n",
    "train_df_with_predCol[predCol]=train_df_with_predCol[predCol]*train_std[predCol]+train_mean[predCol]\n",
    "val_df_with_predCol[predCol]=val_df_with_predCol[predCol]*train_std[predCol]+train_mean[predCol]\n",
    "test_df3[predCol]=test_df3[predCol]*train_std[predCol]+train_mean[predCol]\n",
    "#og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df_with_predCol, val_df=val_df_with_predCol, test_df=test_df3,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df_with_predCol = train_df_with_predCol\n",
    "    self.val_df_with_predCol = val_df_with_predCol\n",
    "    self.test_df_with_predCol= test_df3\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df_with_predCol.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "    print(\"self.input_indices are given by \",self.input_indices)\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "##----------------------->\n",
    "CONV_WIDTH = 3\n",
    "conv_window = WindowGenerator(\n",
    "    input_width=CONV_WIDTH,\n",
    "    label_width=0,\n",
    "    shift=0,\n",
    "    label_columns=['ActualGenerationMW'])\n",
    "\n",
    "conv_window\n",
    "\n",
    "##----------------------->\n",
    "\n",
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df_with_predCol)\n",
    "\n",
    "\n",
    "@property\n",
    "def trainX(self):\n",
    "  return self.make_dataset(self.train_df_with_predColX)\n",
    "\n",
    "@property\n",
    "def trainY(self):\n",
    "  return self.make_dataset(self.train_df_with_predColY)\n",
    "\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.val_df_with_predCol)\n",
    "\n",
    "@property\n",
    "def valX(self):\n",
    "  return self.make_dataset(self.val_df_with_predColX)\n",
    "\n",
    "@property\n",
    "def valY(self):\n",
    "  return self.make_dataset(self.val_df_with_predColY)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_dataset(self.test_df3)\n",
    "\n",
    "@property\n",
    "def testX(self):\n",
    "  return self.make_dataset(self.test_df3X)\n",
    "\n",
    "@property\n",
    "def testY(self):\n",
    "  return self.make_dataset(self.test_df3Y)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.train))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "\n",
    "WindowGenerator.val = val\n",
    "\n",
    "\n",
    "WindowGenerator.test = test\n",
    "\n",
    "\n",
    "\n",
    "WindowGenerator.example = example\n",
    "###================================>\n",
    "\n",
    "def split_window(self, features):\n",
    "    print(\"self.input_slice is \",self.input_slice)\n",
    "    print(\"self.labels_slice is \",self.labels_slice)\n",
    "    inputs = features[:, self.input_slice, :]\n",
    "    labels = features[:, self.labels_slice, :]\n",
    "    print(\"labels are given by\", labels)\n",
    "    print(\"inputs are given by\", inputs)\n",
    "\n",
    "    '''if self.label_columns is not None:\n",
    "        labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)'''\n",
    "\n",
    "    # Slicing doesn't preserve static shape information, so set the shapes\n",
    "    # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "    inputs.set_shape([None, self.input_width, None])\n",
    "    labels.set_shape([None, self.label_width, None])\n",
    "    print(\"labels are now given by\", labels)\n",
    "    print(\"inputs are now given by\", inputs)\n",
    "    return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window\n",
    "\n",
    "###================================>\n",
    "\n",
    "def make_dataset(self, data):\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    print(\"Data is  given by\",data.shape)\n",
    "    \n",
    "    inputData=data[:,:20]\n",
    "    targets=data[:,20:]\n",
    "    print(\"Input data is given by\", inputData.shape)\n",
    "    print(\"targets are given by\",targets.shape)\n",
    "\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        inputData,\n",
    "        targets,\n",
    "        sequence_length=self.total_window_size,\n",
    "        sequence_stride=1,\n",
    "        shuffle=False,\n",
    "        batch_size=32,)\n",
    "    print(\"ds is given by \",ds)\n",
    "    #ds = ds.map(self.split_window)\n",
    "\n",
    "    return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset\n",
    "\n",
    "###================================>\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=patience,mode='min')\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "  print(window.train)\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "  return history\n",
    "\n",
    "###================================>\n",
    "\n",
    "conv_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32,kernel_size=(CONV_WIDTH-1,),activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "])\n",
    "###================================>\n",
    "\n",
    "history = compile_and_fit(conv_model, conv_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['Conv'] = conv_model.evaluate(conv_window.val)\n",
    "performance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(self, model=None, plot_col=predCol[0], max_subplots=3):\n",
    "  inputs, labels = self.example\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plot_col_index = self.column_indices[plot_col]\n",
    "  max_n = min(max_subplots, len(inputs))\n",
    "  for n in range(max_n):\n",
    "    plt.subplot(max_n, 1, n+1)\n",
    "    plt.ylabel(f'{plot_col} [normed]')\n",
    "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "    if self.label_columns:\n",
    "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "    else:\n",
    "      label_col_index = plot_col_index\n",
    "\n",
    "    if label_col_index is None:\n",
    "      continue\n",
    "\n",
    "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "    if model is not None:\n",
    "      predictions = model(inputs)\n",
    "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "\n",
    "    if n == 0:\n",
    "      plt.legend()\n",
    "\n",
    "  plt.xlabel('Time [h]')\n",
    "\n",
    "WindowGenerator.plot = plot\n",
    "\n",
    "\n",
    "\n",
    "LABEL_WIDTH = 24\n",
    "INPUT_WIDTH = LABEL_WIDTH + (CONV_WIDTH - 1)\n",
    "wide_conv_window = WindowGenerator(\n",
    "    input_width=INPUT_WIDTH,\n",
    "    label_width=LABEL_WIDTH,\n",
    "    shift=1,\n",
    "    label_columns=['ActualGenerationMW'])\n",
    "\n",
    "wide_conv_window\n",
    "\n",
    "wide_conv_window.plot(conv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364acb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_with_predCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# Creating predicted column\n",
    "y_predAr=conv_model.predict(conv_window.val)\n",
    "y_pred=y_predAr[:,0]\n",
    "\n",
    "\n",
    "\n",
    "# Setting the values\n",
    "y_valid =        val_df[predCol].iloc[2:]\n",
    "y_forecasted = val_df['ForecastWind'].iloc[2:]\n",
    "\n",
    "val_df_with_predColPlot=val_df_with_predCol.iloc[2:]\n",
    "\n",
    "print(len(y_forecasted))\n",
    "print(len(y_valid))\n",
    "print(len(y_pred))\n",
    "print(len(val_df[predCol]))\n",
    "print(len(val_df_with_predColPlot[predCol]))\n",
    "\n",
    "val_df_with_predColPlot['forecasted']=y_forecasted\n",
    "\n",
    "val_df_with_predColPlot['predicted']=y_pred\n",
    "\n",
    "plt.plot(X_valid['date2'][-7*24:],y_pred[-7*24:],color = 'cyan',label = 'ESB Forecasted Wind Generation')\n",
    "plt.plot(X_valid['date2'][-7*24:],y_valid[-7*24:],color = 'red',label = 'Actual Wind Generation')\n",
    "plt.plot(X_valid['date2'][-7*24:],y_forecasted[-7*24:],color = 'blue', label = 'Kevins Modeled Wind Generation')\n",
    "\n",
    "plt.legend(loc=1)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "fig.savefig('test2png.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef0ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_with_predCol[['forecasted','ActualWind']].diff().abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_with_predColPlot[['forecasted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa8e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(val_df_with_predColPlot[predCol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(abs(np.array(val_df_with_predColPlot[['forecasted']])-np.array(val_df_with_predColPlot[predCol])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01054116",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(abs(np.array(val_df_with_predColPlot[['predicted']])-np.array(val_df_with_predColPlot[predCol])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b78474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_with_predCol[['ActualWind','predicted']].diff().abs().mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
